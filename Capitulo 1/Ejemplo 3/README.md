
# Mini fine-tuning de GPT-2 (espa√±ol) con ~100 l√≠neas de *Don Quijote*

> Ajuste educativo **ultra-ligero** de un GPT-2 peque√±o en espa√±ol para **completar un cuento de 5 oraciones**.  
> Dise√±ado para **Google Colab** (GPU recomendada) o ejecuci√≥n local.

---

## üß≠ Qu√© hace este repo

1. Descarga **100 l√≠neas** de *Don Quijote* (dominio p√∫blico) desde Project Gutenberg. 
2. Prepara un **dataset m√≠nimo** (solo para demostraci√≥n did√°ctica).  
3. Ajusta el modelo **`datificate/gpt2-small-spanish`** (licencia Apache-2.0 seg√∫n su model card). 
4. Genera un **cuento de 5 oraciones** con sampling controlado.  
5. Guarda el modelo ajustado en `gpt2_es_quijote_minift/final/`.

> Para los curiosos: el procedimiento sigue la **receta oficial** de *Causal Language Modeling* de Transformers (secci√≥n ‚ÄúLanguage modeling‚Äù).

---

## ‚ö°Ô∏è Ejecuci√≥n en Google Colab (recomendada)

1. **Abrir Colab** y activar GPU: `Entorno de ejecuci√≥n ‚Üí Cambiar tipo de entorno ‚Üí GPU`.
2. Crear una celda e instalar dependencias:
    ```bash
    pip install "transformers==4.45.2" "datasets==2.20.0" "accelerate==0.33.0" sentencepiece
    ```

3. **Sube** `gpt2_es_minift.py` al espacio de trabajo de Colab.
4. Ejecuta:

   ```bash
   python gpt2_es_minift.py --epochs 2 --lr 1e-4 --block_size 128 --max_new_tokens 160
   ```

**Salida esperada (resumen):**

* M√©trica `eval_loss` y su **perplejidad aproximada**.
* Un **cuento de 5 oraciones** con rasgos estil√≠sticos del corpus breve (puede sonar arcaizante).

---

## üñ•Ô∏è Ejecuci√≥n local

Requisitos:

* Python **3.9+** (probado en 3.10)
* GPU NVIDIA (opcional) con drivers/CUDA configurados

```bash
python -m venv .venv && source .venv/bin/activate  # en Windows: .venv\Scripts\activate
pip install "transformers==4.45.2" "datasets==2.20.0" "accelerate==0.33.0" sentencepiece
python gpt2_es_minift.py
```

---

## üîß Par√°metros √∫tiles (CLI)

| Flag                   | Default                         | Explicaci√≥n ‚Äúamigable‚Äù                                                      |
| ---------------------- | ------------------------------- | --------------------------------------------------------------------------- |
| `--epochs`             | `2`                             | **Vueltas al cuaderno**; con pocos datos, 1‚Äì3 suelen bastar.                |
| `--lr`                 | `1e-4`                          | **Qu√© tan r√°pido corrijo**; muy grande = inestable, muy peque√±o = lento.    |
| `--block_size`         | `128`                           | **Ventana de contexto** por lote; subirlo consume m√°s VRAM.                 |
| `--train_bs`           | `2`                             | Tama√±o de batch (train) por dispositivo.                                    |
| `--grad_accum`         | `4`                             | **Acumulaci√≥n de gradientes** para simular batches m√°s grandes.             |
| `--max_new_tokens`     | `160`                           | Largo m√°ximo del texto **nuevo** a generar.                                 |
| `--temperature`        | `0.9`                           | Creatividad: ‚Üë temperatura ‚áí ‚Üë variedad.                                    |
| `--top_k` / `--top_p`  | `50` / `0.95`                   | Sampling controlado: **suerte con cintur√≥n de seguridad**.                  |
| `--repetition_penalty` | `1.1`                           | Penaliza repeticiones molestas.                                             |
| `--model_id`           | `datificate/gpt2-small-spanish` | Puedes cambiar a otro GPT-2 en espa√±ol si lo prefieres. ([Hugging Face][1]) |

---

## üß™ ¬øQu√© aprendizaje ofrece?

* **Transferencia de estilo** con **muy pocos datos**: ver√°s vocabulario y cadencia ‚Äúquijotesca‚Äù.
* **Limitaciones**: el dataset diminuto propicia **sobreajuste**; no es un experimento de producci√≥n.
* **Buena pr√°ctica**: compara generaciones **antes** y **despu√©s** del ajuste y pide a tus estudiantes identificar ‚Äúhuellas‚Äù del corpus.

Para extender:

* Mezcla textos contempor√°neos para un tono m√°s moderno.
* Incrementa `epochs` y reduce `lr` para un ajuste m√°s fino (si tienes m√°s datos).
* Explora kernels/optimizaciones modernas si escalas (p. ej., *FlashAttention*). ([arXiv][2], [ar5iv][3], [GitHub][4])

---

## üß∞ Estructura de archivos

```
.
‚îú‚îÄ‚îÄ gpt2_es_minift.py          # Script principal (entrena, eval√∫a, genera y guarda)
‚îú‚îÄ‚îÄ don_quijote_100.txt        # Se crea autom√°ticamente (100 l√≠neas extra√≠das)
‚îî‚îÄ‚îÄ gpt2_es_quijote_minift/
    ‚îî‚îÄ‚îÄ final/                 # Carpeta del modelo ajustado + tokenizer
```

---

## üßë‚Äçüè´ Consejos docentes (10 min, sin c√≥digo)

**Actividad ‚Äúantes/despu√©s‚Äù**

1. Genera 2‚Äì3 p√°rrafos **antes** del ajuste y 2‚Äì3 **despu√©s**.
2. En parejas, pidan a los estudiantes resaltar cambios de **l√©xico**, **ritmo** y **giros**.
3. Cierre: ¬øqu√© perdimos/ganamos con el *fine-tuning*? Relaci√≥n con **atenci√≥n** y **contexto**.

---

## üîí Licencias y procedencia de datos/modelo

* **Texto**: *Don Quijote* desde Project Gutenberg (*2000-0.txt*). Ver t√©rminos en la p√°gina del proyecto. ([Project Gutenberg][5])
* **Modelo base**: \[`datificate/gpt2-small-spanish`] (model card indica **Apache-2.0**). Revisa siempre el model card para uso y atribuci√≥n. ([Hugging Face][6])
* **C√≥digo de este repo**: MIT (aj√∫stalo si tu instituci√≥n requiere otra licencia).

---

## üÜò Soluci√≥n de problemas

* **OOM / fuera de memoria**: baja `--train_bs` a `1` o `--block_size` a `64`; tambi√©n puedes desactivar GPU y correr en CPU (m√°s lento).
* **Texto repetitivo**: sube `--repetition_penalty` a `1.2‚Äì1.3`, o ajusta `--top_p` a `0.97`.
* **Genera menos de 5 oraciones**: aumenta `--max_new_tokens` (p. ej., 220) o baja `--temperature` (0.8).

---

## üìö Lecturas recomendadas

* *Causal Language Modeling* (Transformers Docs). ([Hugging Face][7])
* Curso HF: *Training a causal language model from scratch* (cap. 7.6). ([Hugging Face][8])
* *FlashAttention* (para escalar eficientemente). ([arXiv][2])



---

## ‚ù§Ô∏è Cr√©ditos

* Hugging Face Transformers ‚Äî gu√≠a de *language modeling*. ([Hugging Face][7])
* Project Gutenberg ‚Äî *Don Quijote* (ed. UTF-8). ([Project Gutenberg][5])
* Modelo `datificate/gpt2-small-spanish` (ver model card/licencia). ([Hugging Face][6])



[1]: https://huggingface.co/datificate/gpt2-small-spanish/tree/main "datificate/gpt2-small-spanish at main"
[2]: https://arxiv.org/abs/2205.14135 "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
[3]: https://ar5iv.labs.arxiv.org/html/2205.14135 "Fast and Memory-Efficient Exact Attention with IO-Awareness"
[4]: https://github.com/Dao-AILab/flash-attention "Dao-AILab/flash-attention: Fast and memory-efficient ..."
[5]: https://www.gutenberg.org/files/2000/2000-0.txt "El ingenioso hidalgo don Quijote de la Mancha"
[6]: https://huggingface.co/datificate/gpt2-small-spanish "datificate/gpt2-small-spanish"
[7]: https://huggingface.co/docs/transformers/en/tasks/language_modeling "Causal language modeling"
[8]: https://huggingface.co/learn/llm-course/en/chapter7/6 "Training a causal language model from scratch"
