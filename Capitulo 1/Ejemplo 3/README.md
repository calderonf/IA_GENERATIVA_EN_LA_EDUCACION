
# Mini fine-tuning de GPT-2 (espa√±ol) con ~100 l√≠neas de *Don Quijote*

> Ajuste educativo **ultra-ligero** de un GPT-2 peque√±o en espa√±ol para **completar un prompt corto**.  
> Dise√±ado para **Google Colab** (GPU recomendada) o ejecuci√≥n local.

---

## üß≠ Qu√© hace este repo

1. Descarga **algunas l√≠neas** de *Don Quijote* (dominio p√∫blico) desde Project Gutenberg. 
2. Prepara un **dataset m√≠nimo** (solo para demostraci√≥n did√°ctica).  
3. Ajusta el modelo **`datificate/gpt2-small-spanish`** (licencia Apache-2.0 seg√∫n su model card). 
4. Genera un **cuento de 5 oraciones** con muestreo controlado.  
5. Guarda el modelo ajustado en `gpt2_es_quijote_minift/final/`.

> Tambien puedes encontrar este ejemplo directamente en: [google colab](https://colab.research.google.com/drive/1uSRsAuAs2PqXKouHunf8URDaG1rNv-Ut?usp=sharing)

> Para los curiosos: el procedimiento sigue la **receta oficial** de *Causal Language Modeling* de Transformers (secci√≥n ‚ÄúLanguage modeling‚Äù), ver al final en cr√©ditos.

---

## ‚ö°Ô∏è Ejecuci√≥n en Google Colab (recomendada)

1. **Abrir Colab** y activar GPU: `Entorno de ejecuci√≥n ‚Üí Cambiar tipo de entorno de ejecuci√≥n ‚Üí GPU T4` o alguna otra que tenga disponible que tenga las letras GPU NO CPU o TPU, y dar click en `Guargar` .
2. Crear una celda `+C√≥digo` y copiar y pegar el c√≥digo de miniift.py en esta celda
3. Ejecuta: dando click en `Ejecutar todas` o en el simbolo de play que sale al acercar el mouse a la parte superior izquierda de la celda que quieres ejecutar
4. Puedes crear otra celda y ejecutar:
    ```python
    gen = load_generator_from_disk()
    generate_text("Escribe cinco oraciones sobre un colegio de monjas quijotescas.", generator=gen)#ac√° el prompt entre ""
    ```

**Salida esperada (resumen):**

* M√©trica `eval_loss` y su **perplejidad aproximada**.
* Un **cuento de 5 oraciones** con rasgos estil√≠sticos del corpus breve (puede sonar arcaizante).

---


## üîß Par√°metros √∫tiles (CLI)
En la parte superior del script puedes ajustar directamente las variables.

| Variable             | Default     | Explicaci√≥n ‚Äúamigable‚Äù                                           |
| -------------------- | ----------- | ---------------------------------------------------------------- |
| `EPOCHS`             | `2`         | **Vueltas al cuaderno**; con pocos datos, 1‚Äì3 bastan.            |
| `LR`                 | `1e-4`      | **Qu√© tan r√°pido corrijo**; grande = inestable, peque√±o = lento. |
| `BLOCK_SIZE`         | `128`       | **Ventana de contexto**; m√°s grande consume m√°s VRAM.            |
| `TRAIN_BS`           | `2`         | Tama√±o de batch (train).                                         |
| `GRAD_ACCUM`         | `4`         | **Acumulaci√≥n de gradientes** (simula batches mayores).          |
| `MAX_NEW_TOKENS`     | `160`       | Largo m√°ximo del texto **nuevo**.                                |
| `TEMPERATURE`        | `0.9`       | Creatividad: ‚Üë temperatura ‚áí ‚Üë variedad.                         |
| `TOP_K` / `TOP_P`    | `50`/`0.95` | Sampling controlado: **suerte con cintur√≥n de seguridad**.       |
| `REPETITION_PENALTY` | `1.1`       | Penaliza repeticiones molestas.                                  |

---

## üß™ ¬øQu√© aprendizaje ofrece?

* **Transferencia de estilo** con **muy pocos datos**: ver√°s vocabulario y cadencia ‚Äúquijotesca‚Äù.
* **Limitaciones**: el dataset diminuto propicia **sobreajuste**; no es un experimento de producci√≥n.
* **Buena pr√°ctica**: compara generaciones **antes** y **despu√©s** del ajuste y pide a tus estudiantes identificar ‚Äúhuellas‚Äù del corpus con el que se est√° entrenando aumenta N_LINES = 500  al inicio para entrenar con algo m√°s del corpus, que pasa?

Para extender:

* Mezcla textos contempor√°neos para un tono m√°s moderno.
* Incrementa `EPOCHS` y reduce `LR` para un ajuste m√°s fino (si tienes m√°s datos).
* Explora kernels/optimizaciones modernas si escalas (p. ej., *FlashAttention*). ([arXiv][2], [ar5iv][3], [GitHub][4]) (avanzado)

---

## üß∞ Estructura de archivos

```
.
‚îú‚îÄ‚îÄ gpt2_es_minift.py          # Script principal (entrena, eval√∫a, genera y guarda)
‚îú‚îÄ‚îÄ don_quijote_100.txt        # Se crea autom√°ticamente
‚îî‚îÄ‚îÄ gpt2_es_quijote_minift/
    ‚îî‚îÄ‚îÄ final/                 # Carpeta del modelo ajustado + tokenizer
```

---

## üßë‚Äçüè´ Consejos docentes (10 min, sin c√≥digo)

**Actividad ‚Äúantes/despu√©s‚Äù**

1. Genera 2‚Äì3 p√°rrafos **antes** del ajuste y 2‚Äì3 **despu√©s**.
2. En parejas, pidan a los estudiantes resaltar cambios de **l√©xico**, **ritmo** y **giros**.
3. Cierre: ¬øqu√© perdimos/ganamos con el *fine-tuning*? Relaci√≥n con **atenci√≥n** y **contexto**.

---

## üîí Licencias y procedencia de datos/modelo

* **Texto**: *Don Quijote* desde Project Gutenberg (*2000-0.txt*). Ver t√©rminos en la p√°gina del proyecto. ([Project Gutenberg][5])
* **Modelo base**: \[`datificate/gpt2-small-spanish`] (model card indica **Apache-2.0**). Revisa siempre el model card para uso y atribuci√≥n. ([Hugging Face][6])
* **C√≥digo de este repo**: MIT .

---

## üÜò Soluci√≥n de problemas

* **OOM / fuera de memoria**: baja `--train_bs` a `1` o `--block_size` a `64`; tambi√©n puedes desactivar GPU y correr en CPU (m√°s lento).
* **Texto repetitivo**: sube `--repetition_penalty` a `1.2‚Äì1.3`, o ajusta `--top_p` a `0.97`.
* **Genera menos de 5 oraciones**: aumenta `--max_new_tokens` (p. ej., 220) o baja `--temperature` (0.8).

---

## üìö Lecturas recomendadas

* *Causal Language Modeling* (Transformers Docs). ([Hugging Face][7])
* Curso HF: *Training a causal language model from scratch* (cap. 7.6). ([Hugging Face][8])
* *FlashAttention* (para escalar eficientemente). ([arXiv][2])



---

## ‚ù§Ô∏è Cr√©ditos

* Hugging Face Transformers ‚Äî gu√≠a de *language modeling*. ([Hugging Face][7])
* Project Gutenberg ‚Äî *Don Quijote* (ed. UTF-8). ([Project Gutenberg][5])
* Modelo `datificate/gpt2-small-spanish` (ver model card/licencia). ([Hugging Face][6])


## Enlaces y referencias r√°pidas

1. [datificate/gpt2-small-spanish ‚Äì √°rbol completo](https://huggingface.co/datificate/gpt2-small-spanish/tree/main) 
2. [FlashAttention (paper en arXiv)](https://arxiv.org/abs/2205.14135) 
3. [FlashAttention (versi√≥n web ar5iv)](https://ar5iv.labs.arxiv.org/html/2205.14135)
4. [Repositorio oficial de FlashAttention](https://github.com/Dao-AILab/flash-attention)
5. [Don Quijote de la Mancha ‚Äì Proyecto Gutenberg](https://www.gutenberg.org/files/2000/2000-0.txt)
6. [Modelo GPT-2 small en espa√±ol ‚Äì Hugging Face](https://huggingface.co/datificate/gpt2-small-spanish)
7. [Gu√≠a oficial: Causal Language Modeling](https://huggingface.co/docs/transformers/en/tasks/language_modeling)
8. [Curso LLM: entrenar un modelo causal desde cero](https://huggingface.co/learn/llm-course/en/chapter7/6) 
